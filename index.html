<!DOCTYPE html>

<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Padding Tone – Project Page</title>
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:400,700&display=swap">
	<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/css/bootstrap.min.css">
	<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
	<link href="fontawesome-6.6.0/css/all.css" rel="stylesheet">
    <style>
		.btn {
		    padding: 7px 15px;
			font-size: 20px;
		}
		
		mark {
			-webkit-animation: 3s highlight 1.5s 1 normal forwards;
			animation: 3s highlight 1.5s 1 normal forwards;
			background-color: none;
			background: linear-gradient(90deg, #f7f5bc 50%, rgba(255, 255, 255, 0) 50%);
			background-size: 200% 100%;
			background-position: 100% 0;
		}
		
		@-webkit-keyframes highlight {
		  to {
			background-position: 0 0;
		  }
		}

		@keyframes highlight {
		  to {
			background-position: 0 0;
		  }
		}
		
		h3 {
			font-weight: bold;
		}
		
        body {
            font-family: 'Roboto', sans-serif;
            margin: 0;
            padding: 0;
            background-color: #ffffff;
        }

        header {
            background-color: #0A496B;
            color: #fff;
            padding: 60px 0;
            text-align: center;
        }
		
		figure {
		  text-align: center; /* Centers the content inside the figure */
		  margin: 20px auto; /* Adds vertical space and centers the figure horizontally */
		}
		
		figcaption {
		  padding-top: 10px;
		  color: #555;
		  font-style: italic; /* Styling for the caption */
		}

<!--         header h1 {
			margin: 0;
			font-size: 60px;
			padding-bottom: 30px;
			padding-top: 20px;
			padding-left: 20px;
			padding-right: 20px;
        } -->
		
		header h1 {
            margin: 0;
            font-size: 60px;
			padding-bottom: 40px;
        }

        header h2 {
            margin: 10px 0 0;
            font-weight: 400;
        }
		
		header address a {
			font-size: 20px;
			color: #4abef8;
		}
		
		header address {
			color: #337ab7
		}
		
		header address institute {
			color: #fff;
			font-size: 20px;
		}
		
		header address sup {
			color: #fff;
		}

        .container {
            width: 90%;
            max-width: 1200px;
            margin: 20px auto;
        }
		
        section {
            margin-bottom: 50px;
        }

        section h2 {
            font-size: 28px;
            border-left: 5px solid #4a9aac;
            padding-left: 10px;
            color: #333;
        }

        section p {
            font-size: 18px;
            line-height: 1.6;
            margin-top: 10px;
        }
		
		        @media (max-width: 768px) {
            header h1 {
                font-size: 32px;
            }

            header address a {
                font-size: 16px;
            }

            .container {
                width: 100%;
                padding: 0 15px;
            }

            .figure-container figure {
                flex: 1 1 100%;
            }

            .key-contributions, .methodology, .definitions {
                padding: 15px;
            }

            section h2 {
                font-size: 20px;
            }
        }

        @media (max-width: 480px) {
            header h1 {
                font-size: 28px;
            }

            header address a {
                font-size: 14px;
            }

            .key-contributions, .methodology, .definitions {
                padding: 10px;
            }

            section p {
                font-size: 14px;
            }
        }
		
		/* On larger screens (e.g., tablets, desktops) */
		@media (min-width: 768px) {
			img.float-figure {
				float: right;
				margin-left: 20px;
				margin-right: 0;
				width: 30%; /* Adjust based on your layout */
			}
		}

		/* On smaller screens (e.g., phones), reset to inline */
		@media (max-width: 767px) {
			img.float-figure {
				float: none;
				margin-left: auto;
				margin-right: auto;
				width: 100%; /* Full width inline */
			}
		}
		
		.figure-container {
		  display: grid;
		  grid-template-columns: 1fr 1fr; /* Creates two columns of equal width */
		  gap: 20px; /* Space between columns */
		}
		
		.subtitle {
		    font-size: 28px;
            border-left: 5px solid #57a7e6;
            padding-left: 10px;
            color: #333;
		}

        .key-contributions, .methodology {
            background-color: #f9f9f9;
            padding: 10px;
            border-radius: 8px;
            box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
			font-size: 18px;
			padding-left: 40px;
        }
		
		.methodology {
			background-color: #ffffff;
		}
		
		.definitions {
            background-color: #ededed;
            padding: 20px;
            border-radius: 8px;
            box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
			font-size: 18px;
        }

        .key-contributions h3, .methodology h3 .definitions h3 {
            font-size: 24px;
            color: #444;
        }

        .key-contributions ul, .methodology ul .definitions ul {
            margin-left: 0px;
			font-size: 18px;
        }

        footer {
            background-color: #333;
            color: #fff;
            text-align: center;
            padding: 5px 0;
        }

        footer p {
            margin: 0;
            font-size: 16px;
        }
		.card {
			position: relative;
			display: -webkit-box;
			display: -webkit-flex;
			display: -ms-flexbox;
			display: flex;
			-webkit-box-orient: vertical;
			-webkit-box-direction: normal;
			-webkit-flex-direction: column;
			-ms-flex-direction: column;
			flex-direction: column;
			background-color: #fff;
			border: 1px solid rgba(0, 0, 0, .125);
			border-radius: .25rem;
		.card-header {
			padding: .75rem 1.25rem;
			margin-bottom: 0;
			margin-top: 0;
			background-color: #f7f7f9;
			border-bottom: 1px solid rgba(0, 0, 0, .125);
			}
		.card-block {
			-webkit-box-flex: 1;
			-webkit-flex: 1 1 auto;
			-ms-flex: 1 1 auto;
			flex: 1 1 auto;
			padding: 1.25rem;
		}
		.img-inline {
		  vertical-align: middle;
		  max-width: 100%;
		  height: auto;
		}
    </style>

	<!-- Google tag (gtag.js) -->
	<script async src="https://www.googletagmanager.com/gtag/js?id=G-CGD2FMJ9QY"></script>
	<script>
	  window.dataLayer = window.dataLayer || [];
	  function gtag(){dataLayer.push(arguments);}
	  gtag('js', new Date());
	
	  gtag('config', 'G-CGD2FMJ9QY');
	</script>

</head>
<body>

	<header>
		<h1>Padding Tone:<br>A Mechanistic Analysis of Padding Tokens in T2I Models</h1>
		<address>
			<nobr><a href="https://tokeron.github.io/" target="_blank">Michael Toker</a><sup>1</sup>,</nobr>
			<nobr><a href="https://www.linkedin.com/in/ido-galil/" target="_blank">Ido Galil</a><sup>1,2</sup>,</nobr>
			<nobr><a href="https://orgadhadas.github.io/" target="_blank">Hadas Orgad</a><sup>1</sup>,</nobr>
			<nobr><a href="https://rinongal.github.io/" target="_blank">Rinon Gal</a><sup>2</sup>,</nobr>
			<nobr><a href="https://yoadtew.github.io/" target="_blank">Yoad Tewel</a><sup>2</sup></nobr>,
			<br>
			<nobr><a href="https://chechiklab.biu.ac.il/~gal/" target="_blank">Gal Chechik</a><sup>2,3</sup>,</nobr>
			<nobr><a href="https://belinkov.com/" target="_blank">Yonatan Belinkov</a><sup>1</sup></nobr>
			<br>
			<nobr><sup>1</sup><institute>Technion - IIT</institute></nobr>;
			<nobr><sup>2</sup><institute>Nvidia</institute></nobr>;
			<nobr><sup>3</sup><institute>Bar-Ilan University</institute></nobr>
		</address>
		<a href="https://arxiv.org/pdf/2501.06751" target="_blank" class="btn" style="color: #fff; background-color: #198754; border-color: #136e44;"><i class="ai ai-arxiv"></i> ArXiv</a>
		<a href="https://github.com/padding-tone/padding-tone.github.io/blob/main/2501.06751v1_compressed.pdf" target="_blank" class="btn" style="color: #fff; background-color: #dc3545; border-color: #b72d3a;"><i class="far fa-file-pdf"></i> PDF</a>
		<a href="https://github.com/technion-cs-nlp/Padding-Tone" target="_blank" class="btn" style="color: #fff; background-color: #212529; border-color: #212529;"><i class="fab fa-github"></i> Code</a>
	</header>


<div class="container">

	<center>
	<img src="img/flux_pad.jpeg" class="img-inline" style="width:50%; min-width: 300px; height:auto;">
	</center>

	<h2 class="subtitle">Abstract</h2>
    <section>
        <p>
Text-to-image (T2I) diffusion models rely on encoded prompts to guide the image generation process. Typically, these prompts are extended to a fixed length by adding padding tokens before text encoding. <mark>Despite being a default practice, the influence of padding tokens on the image generation process has not been investigated.</mark> In this work, we conduct <mark>the first in-depth analysis of the role padding tokens play in T2I models</mark>. We develop two causal techniques to analyze how information is encoded in the representation of tokens across different components of the T2I pipeline. Using these techniques, we investigate when and how padding tokens impact the image generation process. Our findings reveal three distinct scenarios: padding tokens may affect the model's output during text encoding, during the diffusion process, or be effectively ignored. Moreover, we identify key relationships between these scenarios and the model's architecture (cross or self-attention) and its training process (frozen or trained text encoder). These insights contribute to a deeper understanding of the mechanisms of padding tokens, potentially informing future model design and training practices in T2I systems.
        </p>
    </section>

	<h2 class="subtitle">Key Insights <i class="fas fa-lightbulb"></i></h2>
    <section class="key-contributions">
        <ul>
		<li><strong>Training Process Matters:</strong>Models with frozen text encoders (e.g., Stable Diffusion) tend to ignore padding tokens, while those with trainable text encoders (e.g., LDM and LLaMA-UNet) don’t.</li>
		<li><strong>Architecture Matters:</strong>Multimodal attention architectures (e.g., FLUX, Stable Diffusion 3) can encode meaningful information in unused padding tokens during diffusion. Attention maps show these tokens may store details vital for nuanced visual generation.</li>
		<li><strong>Proximity to Prompt Matters:</strong>Padding tokens closer to prompt tokens encode more significant information. This suggests the role of positional encoding or causal masking in how padding tokens function, especially in models like LLaMA-UNet.</li>
        </ul>
		
	<b> In summary, The padding tokens are used when: </b>
	<ul>
	<li> The text-encoder is being finetuned (not frozen), or -
	<li> The architecture is diffusion transformer (and not diffusion UNET).
	</ul>
    </section>
	
	<h2 class="subtitle">Analysis of Padding in Text Encoding</h2>
    <section class="methodology">
	<h3>Method</h3>
		<figure>
		<img src="img/method.jpeg" class="img-inline" style="width:100%; min-width: 300px; height:auto;">
		<figcaption class="float-figure">Interpreting information within pad tokens in the text encoder. We first encode the full prompt and
an clean pads separately. Next, we keep the tokens we want to interpret and replace all other tokens with clean
pad tokens. We then generate an image conditioned on this mixed representation.</figcaption>
		</figure>
	<p>	
		By analyzing modified versions of the full text embedding, we isolate the effects of padding tokens on the final output. This approach allows us to visually showcase the distinct contributions of prompt tokens versus padding tokens.
		To achieve this, we replace specific tokens in the prompt with "clean" padding tokens—neutral placeholders that contain no information from the original prompt. By comparing images generated from the full representation and its variations, we illustrate how much information is carried by the padding tokens. The following figure demonstrates this process, highlighting the nuanced interplay between padding and prompt tokens in shaping generated images.
	</p>
	<figure>
	<img src="img/pads_llama_sd3.jpeg" class="img-inline" style="width:50%; min-width: 300px; height:auto;">
	</figure>
	
	<h3>Results</h3>
	<p>	
		The way a text encoder is trained has a significant influence on how padding tokens are handled during image generation. In many current T2I models, such as <strong>Stable Diffusion</strong> and <strong>FLUX</strong>, the text encoder is frozen, meaning it is not fine-tuned for the image generation task. Consequently, padding tokens, which are not explicitly trained to contribute meaningfully to the encoded representation, have little to no impact on the generated images.
		</p><p>
		For instance, as shown in our results, images generated using contextualized padding only in these models yield <strong>low CLIP scores</strong>, comparable to those generated with clean padding.
		</p><p>
		In contrast, models like <strong>LDM</strong> and <strong>Lavi-Bridge</strong> adapt the text encoder specifically for image generation by training it on the task, including the effective use of padding tokens. This adaptation enables the text encoder to learn meaningful representations for padding tokens, with <strong>higher CLIP scores</strong> for images generated from padding only.
		</p><p>
		In the fine-tuned models, we also observe a slight degradation in CLIP score when generating an image from the prompt only, indicating that meaningful information is encoded in the padding tokens.
		</p>
		<figure>
		<img src="img/bar_plot.png" class="img-inline" style="width:70%; min-width: 300px; height:auto;">
		</figure>
		
		<p>
		The following figure illustrates how the finetuning process contributes to the role of padding tokens: as we interpolate the scaling factor for the LoRA weights, we see the increased role of the padding tokens.
		</p>
		<figure>
		<img src="img/lora_scaled.jpeg" class="img-inline" style="width:50%; min-width: 300px; height:auto;">
		</figure>
	</p>	
	
    </section>

	<h2 class="subtitle">Analysis of Padding in Diffusion Process</h2>

    <section class="methodology">
	<h3>Method</h3>
	<figure>
	<img src="img/diffusion_causal.jpeg" class="img-inline" style="width:100%; min-width: 300px; height:auto;">
	<figcaption class="float-figure">
Interpreting information within pad tokens in the diffusion model. We perform a diffusion of two prompts simultaneously: the full prompt and an clean pads. During the diffusion, we keep the tokens we want to interpret (here:  the prompt-contextual padding tokens) and replace all other tokens with clean pad tokens. We perform this intervention before each attention block in the diffusion model, through all diffusion steps. We then generate an image conditioned on this mixed representation.
</figcaption>
	</figure>
	<p>
	Padding tokens can also play a role in the diffusion process of T2I models, where models have either cross-attention or multi-modal self-attention. Cross-attention, as seen in Stable Diffusion 2/XL, keeps text representations static while using attention maps to transfer information from text to image patches. In contrast, MM-DiT blocks in models like FLUX and Stable Diffusion 3, allow both text and image representations to dynamically influence each other, potentially embedding additional information, even into padding tokens. This distinction raises questions about the use and significance of padding tokens across different attention mechanisms.
	</p>
	<p>
	To investigate, we analyze the attention maps between image patches and text tokens. We further perform a causal analysis where we patch clean padding tokens into the contextual padding tokens, as seen in the figure above.
	</p>
	
	
	<h3>Results</h3>
	
	Our findings reveal that while Stable Diffusion XL focuses attention primarily on meaningful tokens, attention maps for FLUX diffusion show strong alignment between prompt tokens and semantically relevant image tokens. These maps also reveal high attention for padding tokens with the main objects in the image.
	
		<figure>
	<img src="img/token_attention_maps.jpeg" class="img-inline" style="width:100%; min-width: 300px; height:auto;">
		<figcaption class="float-figure">
Attention maps for FLUX diffusion show strong alignment between prompt tokens and semantically relevant image tokens. These maps also reveal high attention for padding tokens with the main objects in the image.	</figcaption>
	</figure>
	
	<p>
	Qualitative experiments further showed that removing padding tokens in FLUX often resulted in missing image details, while Stable Diffusion XL maintained consistent output. These insights suggest that models using MM-DiT blocks might leverage padding tokens as temporary storage to pass information during the diffusion process, similar to how vision-language models handle image patches.
	</p>
	
			<figure>
	<img src="img/paddingtone_and_bunny.png" class="img-inline" style="width:40%; min-width: 300px; height:auto;">
		<figcaption class="float-figure">
    Images generated with FLUX from different prompt segments show distinct alignments: prompt tokens produce semantically accurate images, while the visual features of the setting (windows, furniture in the background) emerges only from the prompt-contextual pad tokens.	</figure>
	
	</section>

	<h2 class="subtitle">How to cite</h2>

	<div class="card">
	<h3 class="card-header">bibliography</h3>
	<div class="card-block">
	<p style="text-indent: -3em; margin-left: 3em;" class="card-text clickselect">
	Michael Toker, Ido Galil, Hadas Orgad, Rinon Gal, Yoad Tewel, Gal Chechik, Yonatan Belinkov, “<em>Padding Tone: A Mechanistic Analysis of Padding Tokens in T2I Models</em>”.
	</p>
	</div>
	<h3 class="card-header">bibtex</h3>
	<div class="card-block">
	<pre class="card-text clickselect">
@misc{toker2025paddingtonemechanisticanalysis,
      title={Padding Tone: A Mechanistic Analysis of Padding Tokens in T2I Models}, 
      author={Michael Toker and Ido Galil and Hadas Orgad and Rinon Gal and Yoad Tewel and Gal Chechik and Yonatan Belinkov},
      year={2025},
      eprint={2501.06751},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2501.06751}, 
}
	</pre>
	</div>
	</div>
	<p></p>
</div>


<footer>
    <p>Created by Hadas Orgad | Technion | 2024</p>
</footer>

</body>
</html>
